{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    # c1: 1x1 conv channel, \n",
    "    # c2: 1x1 and 3x3 conv channel, \n",
    "    # c3: 1x1 and 5x5 conv channel, \n",
    "    # c4: 3x3 max-pooling and 1x1 conv channel\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4):\n",
    "        super(Inception, self).__init__()\n",
    "        self.RELU = nn.ReLU()\n",
    "        # path1: 1x1 conv\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # path2: 1x1 and 3x3 conv\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "\n",
    "        # path3: 1x1 and 5x5 conv\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # path4: 3x3 max-pooling and 1x1 conv\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        # path5: 1x1 and 5x5 conv\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "\n",
    "        # path6: 3x3 max-pooling and 1x1 conv\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.RELU(self.p1_1(x))\n",
    "        p2 = self.RELU(self.p2_2(self.RELU(self.p2_1(x))))\n",
    "        p3 = self.RELU(self.p3_2(self.RELU(self.p3_1(x))))\n",
    "        p4 = self.RELU(self.p4_2(self.p4_1(x)))\n",
    "        # batch, channel, height, width\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "        \n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, Inception, in_channels, num_classes=1000):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.b3 = nn.Sequential(\n",
    "            Inception(in_channels=192, c1=64, c2=(96, 128), c3=(16, 32), c4=32),\n",
    "            Inception(in_channels=256, c1=128, c2=(128, 192), c3=(32, 96), c4=64),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.b4 = nn.Sequential(\n",
    "            Inception(in_channels=480, c1=192, c2=(96, 208), c3=(16, 48), c4=64),\n",
    "            Inception(in_channels=512, c1=160, c2=(112, 224), c3=(24, 64), c4=64),\n",
    "            Inception(in_channels=512, c1=128, c2=(128, 256), c3=(24, 64), c4=64),\n",
    "            Inception(in_channels=512, c1=112, c2=(144, 288), c3=(32, 64), c4=64),\n",
    "            Inception(in_channels=528, c1=256, c2=(160, 320), c3=(32, 128), c4=128),    \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.b5 = nn.Sequential(\n",
    "            Inception(in_channels=832, c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "            Inception(in_channels=832, c1=384, c2=(192, 384), c3=(48, 128), c4=128),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "        # 权重初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062193bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def matplot_train_process_data(train_process_data):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_process_data['epoch'], train_process_data['train_loss'], 'ro-',label='Train Loss')\n",
    "    plt.plot(train_process_data['epoch'], train_process_data['val_loss'], 'bs-', label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Val Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_process_data['epoch'], train_process_data['train_acc'], 'ro-', label='Train Acc')\n",
    "    plt.plot(train_process_data['epoch'], train_process_data['val_acc'], 'bs-', label='Val Acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Train and Val Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "import copy\n",
    "import time\n",
    "import torchvision.transforms as Transforms\n",
    "import torchvision.datasets as Datasets\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train_val_data_process():\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_data = Datasets.FashionMNIST(root='./data', train=True, \n",
    "        transform=Transforms.Compose([\n",
    "            Transforms.Resize((224, 224)),\n",
    "            Transforms.ToTensor()\n",
    "        ]),\n",
    "        download=True)\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    train_data, val_data = Data.random_split(train_data, \n",
    "    [round(len(train_data) * 0.8), round(len(train_data) * 0.2)])\n",
    "    \n",
    "    train_loader = Data.DataLoader(dataset=train_data, \n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    val_loader = Data.DataLoader(dataset=val_data, \n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def train_model_process(model, train_loader, val_loader, num_epochs=5):\n",
    "    # 加载模型\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"当前设备：\", device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    # 训练损失\n",
    "    train_loss_all = []\n",
    "    # 训练准确率\n",
    "    train_acc_all = []\n",
    "    # 验证损失\n",
    "    val_loss_all = []\n",
    "    # 验证准确率\n",
    "    val_acc_all = []\n",
    "\n",
    "    since = time.time()\n",
    "    # 训练\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - LR: {optimizer.param_groups[0]['lr']}\")\n",
    "        print('-' * 10)\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_corrects = 0.0\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0.0\n",
    "\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "\n",
    "        # 训练\n",
    "        for step, (b_x, b_y) in enumerate(train_loader):\n",
    "            # 数据移动到设备\n",
    "            b_x = b_x.to(device)\n",
    "            b_y = b_y.to(device)\n",
    "\n",
    "            train_num += b_x.size(0)\n",
    "\n",
    "            # 模型训练\n",
    "            model.train()\n",
    "            outputs = model(b_x)\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, b_y)\n",
    "            \n",
    "            # 梯度清零，防止梯度叠加\n",
    "            optimizer.zero_grad()\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "            # 计算准确率\n",
    "            pre_lab = torch.argmax(outputs, dim=1)\n",
    "            train_corrects += torch.sum(pre_lab == b_y)\n",
    "            # 计算损失\n",
    "            train_loss += loss.item() * b_x.size(0)\n",
    "            \n",
    "            # 计算验证损失和准确率\n",
    "        for step, (b_x, b_y) in enumerate(val_loader):\n",
    "            b_x = b_x.to(device)\n",
    "            b_y = b_y.to(device)\n",
    "\n",
    "            val_num += b_x.size(0)\n",
    "\n",
    "            # 模型验证\n",
    "            model.eval()\n",
    "            outputs = model(b_x)\n",
    "            loss = criterion(outputs, b_y)\n",
    "\n",
    "            # 计算验证损失\n",
    "            val_loss += loss.item() * b_x.size(0)\n",
    "            pre_lab = torch.argmax(outputs, dim=1)\n",
    "            val_corrects += torch.sum(pre_lab == b_y)\n",
    "\n",
    "        # 计算训练损失和准确率\n",
    "        train_loss_all.append(train_loss / train_num)\n",
    "        train_acc_all.append(train_corrects.double().item() / train_num)\n",
    "        val_loss_all.append(val_loss / val_num)\n",
    "        val_acc_all.append(val_corrects.double().item() / val_num)\n",
    "\n",
    "        print('Train loss: {:.4f} Train acc: {:.4f} | Val loss: {:.4f} Val acc: {:.4f}'\n",
    "        .format(train_loss_all[-1], train_acc_all[-1], val_loss_all[-1], val_acc_all[-1]))\n",
    "\n",
    "        # 保存最佳模型参数\n",
    "    if val_acc_all[-1] > best_acc:\n",
    "        best_acc = val_acc_all[-1]\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"训练完成，用时 {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    torch.save(best_model_wts, 'model/vgg16_best_model.pth')\n",
    "\n",
    "    train_process_data = pd.DataFrame({\n",
    "        'epoch': range(num_epochs),\n",
    "        'train_loss': train_loss_all,\n",
    "        'train_acc': train_acc_all,\n",
    "        'val_loss': val_loss_all,\n",
    "        'val_acc': val_acc_all\n",
    "    })\n",
    "    return train_process_data\n",
    "    \n",
    "\n",
    "model = GoogLeNet(Inception, 1, 10)\n",
    "train_loader, val_loader = train_val_data_process()\n",
    "train_process_data = train_model_process(model, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "matplot_train_process_data(train_process_data)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
