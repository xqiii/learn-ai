{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cztk6uq93z",
   "source": "# 简单的 RNN 文本生成\n\n本 notebook 实现了一个基于字符级别的 RNN 模型，用于文本生成。\n\n**主要内容：**\n- 数据准备和预处理\n- RNN 模型定义\n- 训练过程\n- 文本生成测试",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "stz8ewcg32",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\n\n# 设置随机种子\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# 设置设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'使用设备: {device}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9pnpahdi13h",
   "source": "## 1. 数据准备\n\n使用简单的文本数据作为训练语料",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7v2wygplotj",
   "source": "# 训练语料：简单的英文诗句\ntext = \"\"\"\nTo be or not to be that is the question\nWhether tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\nAnd by opposing end them to die to sleep\nNo more and by a sleep to say we end\nThe heartache and the thousand natural shocks\nThat flesh is heir to tis a consummation\nDevoutly to be wished to die to sleep\nTo sleep perchance to dream ay there is the rub\n\"\"\".strip().lower()\n\nprint(f'文本长度: {len(text)} 个字符')\nprint(f'文本预览: {text[:80]}...')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wvf8x1yjaf",
   "source": "# 构建词汇表\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# 创建字符到索引和索引到字符的映射\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_char = {i: ch for i, ch in enumerate(chars)}\n\nprint(f'词汇表大小: {vocab_size}')\nprint(f'字符集: {\"\".join(chars)}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "rvd24zu9st",
   "source": "# 将文本编码为数字\ndata = [char_to_idx[ch] for ch in text]\n\n# 准备训练数据：使用滑动窗口生成序列对\nseq_length = 25  # 序列长度\nX_data = []\ny_data = []\n\nfor i in range(len(data) - seq_length):\n    X_data.append(data[i:i+seq_length])\n    y_data.append(data[i+1:i+seq_length+1])\n\nX_data = np.array(X_data)\ny_data = np.array(y_data)\n\nprint(f'训练样本数: {len(X_data)}')\nprint(f'输入形状: {X_data.shape}')\nprint(f'输出形状: {y_data.shape}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s5deon0a8z9",
   "source": "## 2. 定义 RNN 模型\n\n构建一个简单的字符级 RNN 模型",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mqhbugl3yhd",
   "source": "class CharRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n        super(CharRNN, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        # 嵌入层\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # RNN 层\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n        \n        # 全连接输出层\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n    \n    def forward(self, x, hidden=None):\n        # x: (batch_size, seq_length)\n        batch_size = x.size(0)\n        \n        # 嵌入: (batch_size, seq_length, embedding_dim)\n        embedded = self.embedding(x)\n        \n        # 初始化隐藏状态\n        if hidden is None:\n            hidden = self.init_hidden(batch_size)\n        \n        # RNN 前向传播\n        rnn_out, hidden = self.rnn(embedded, hidden)\n        # rnn_out: (batch_size, seq_length, hidden_dim)\n        \n        # 输出层\n        output = self.fc(rnn_out)\n        # output: (batch_size, seq_length, vocab_size)\n        \n        return output, hidden\n    \n    def init_hidden(self, batch_size):\n        # 初始化隐藏状态: (num_layers, batch_size, hidden_dim)\n        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n\n# 模型超参数\nembedding_dim = 50\nhidden_dim = 128\nnum_layers = 2\n\n# 创建模型\nmodel = CharRNN(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\nprint(f'模型参数量: {sum(p.numel() for p in model.parameters())}')\nprint(model)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rt21jkaouem",
   "source": "## 3. 训练模型",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pf1rlq8fjb8",
   "source": "# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.005)\n\n# 训练参数\nnum_epochs = 500\nbatch_size = 32\n\n# 将数据转换为 PyTorch 张量\nX_train = torch.LongTensor(X_data).to(device)\ny_train = torch.LongTensor(y_data).to(device)\n\n# 训练循环\nlosses = []\nmodel.train()\n\nfor epoch in range(num_epochs):\n    # 随机打乱数据\n    indices = torch.randperm(len(X_train))\n    epoch_loss = 0\n    \n    # 按批次训练\n    for i in range(0, len(X_train), batch_size):\n        batch_indices = indices[i:i+batch_size]\n        X_batch = X_train[batch_indices]\n        y_batch = y_train[batch_indices]\n        \n        # 前向传播\n        output, _ = model(X_batch)\n        \n        # 计算损失\n        # output: (batch_size, seq_length, vocab_size)\n        # y_batch: (batch_size, seq_length)\n        loss = criterion(output.reshape(-1, vocab_size), y_batch.reshape(-1))\n        \n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    # 记录平均损失\n    avg_loss = epoch_loss / (len(X_train) // batch_size + 1)\n    losses.append(avg_loss)\n    \n    # 每 50 个 epoch 打印一次\n    if (epoch + 1) % 50 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n\nprint('训练完成！')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4n96eeu9n7d",
   "source": "# 可视化损失曲线\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('训练损失曲线')\nplt.grid(True)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vtlzbsd32nc",
   "source": "## 4. 文本生成\n\n使用训练好的模型生成新文本",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qonbpy9a83c",
   "source": "def generate_text(model, start_text, length=200, temperature=0.8):\n    \"\"\"\n    生成文本\n    \n    参数:\n        model: 训练好的模型\n        start_text: 起始文本\n        length: 生成的字符数\n        temperature: 温度参数，控制随机性（值越大越随机）\n    \"\"\"\n    model.eval()\n    \n    # 准备输入\n    chars_generated = list(start_text.lower())\n    input_seq = [char_to_idx[ch] for ch in start_text.lower()]\n    \n    # 初始化隐藏状态\n    hidden = None\n    \n    with torch.no_grad():\n        for _ in range(length):\n            # 转换为张量\n            x = torch.LongTensor([input_seq]).to(device)\n            \n            # 前向传播\n            output, hidden = model(x, hidden)\n            \n            # 获取最后一个时间步的输出\n            last_output = output[0, -1, :]\n            \n            # 应用温度\n            last_output = last_output / temperature\n            \n            # 使用 softmax 获取概率分布\n            probs = torch.softmax(last_output, dim=0).cpu().numpy()\n            \n            # 根据概率分布采样下一个字符\n            next_char_idx = np.random.choice(len(probs), p=probs)\n            next_char = idx_to_char[next_char_idx]\n            \n            # 添加到生成的文本\n            chars_generated.append(next_char)\n            \n            # 更新输入序列（只保留最后 seq_length 个字符）\n            input_seq.append(next_char_idx)\n            input_seq = input_seq[-seq_length:]\n    \n    return ''.join(chars_generated)\n\nprint('文本生成函数已定义')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ubg7p8dn26e",
   "source": "# 测试 1: 从 \"to be\" 开始生成\nprint(\"=\" * 50)\nprint(\"测试 1: 从 'to be' 开始生成文本\")\nprint(\"=\" * 50)\ngenerated = generate_text(model, \"to be\", length=150, temperature=0.5)\nprint(generated)\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a1ugj33imvo",
   "source": "# 测试 2: 从 \"to sleep\" 开始生成，使用更高温度（更随机）\nprint(\"=\" * 50)\nprint(\"测试 2: 从 'to sleep' 开始生成文本（高温度，更随机）\")\nprint(\"=\" * 50)\ngenerated = generate_text(model, \"to sleep\", length=150, temperature=1.0)\nprint(generated)\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7lbw5cke5j3",
   "source": "# 测试 3: 从 \"the\" 开始生成，使用低温度（更确定性）\nprint(\"=\" * 50)\nprint(\"测试 3: 从 'the' 开始生成文本（低温度，更确定性）\")\nprint(\"=\" * 50)\ngenerated = generate_text(model, \"the\", length=150, temperature=0.3)\nprint(generated)\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cmf7dzno3or",
   "source": "## 总结\n\n### 模型架构\n- **输入**: 字符序列（编码为整数）\n- **嵌入层**: 将字符索引转换为稠密向量\n- **RNN 层**: 2 层 RNN，隐藏维度 128\n- **输出层**: 全连接层，输出词汇表大小的 logits\n\n### 关键参数\n- `seq_length=25`: 输入序列长度\n- `embedding_dim=50`: 嵌入维度\n- `hidden_dim=128`: RNN 隐藏层维度\n- `num_layers=2`: RNN 层数\n- `temperature`: 控制生成文本的随机性\n  - 低温度（0.3）：更保守，倾向于选择高概率字符\n  - 高温度（1.0）：更随机，增加多样性\n\n### 改进方向\n1. 使用更大的训练语料\n2. 尝试 LSTM 或 GRU 替代简单 RNN\n3. 增加模型层数和隐藏维度\n4. 实现词级别（而非字符级别）的模型\n5. 添加 dropout 防止过拟合\n6. 使用学习率调度器",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}