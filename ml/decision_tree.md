# 决策树

## 算法原理

策树的基本思想是根据某种原则每次选择一个属性作为划分依据，然后按属性的取值将数据集中的样本进行划分。从逻辑角度，一堆 if else语句的组合。从几何角度，根据某种准则划分特征空间。最终目的：将样本越分越“纯”。



## ID3决策树

### 信息熵

自信息：

$$I(X) = -log_b p(x)$$

当 $b = 2$ 时单位为 $bit$，当 $b = e$ 时单位为 $nat$。

信息熵（自信息的期望）：

度量随机变量 $X$ 的不确定性，信息熵越大越不确定。

$$H(X) = E[I(X)] = - \sum_x \, p(x) \, log_b \, p(x)$$

若 $p(x) = 0$，则 $p(x)log_b p(x) = 0$。当 $X$ 某个取值的概率为1时信息熵最小（最确定），其值为0；当 $X$ 的各个取值的概率等时信息熵最大（最不确定），其值为 $log_b|X|$，其中 $|X|$ 表示 $X$ 可能取值的个数。


将样本类别标记 $y$ 视作随机变量，各个类别在样本集合 $D$ 中的占比 $p_k \; (k = 1,2,\cdots,|Y|)$ 视作各个类别取值的概率，则样本集合 $D$ （随机变量 $y$）的信息熵为：

$$
Ent(D) = - \sum_{k=1}^{|Y|} p_k \, log_2 \, p_k
$$

此时的信息熵代表的不确定性可以转换理解为集合样本的“纯度”。


### 条件熵

$Y$ 的信息熵关于概率分布X的期望：在已知 $X$ 后 $Y$ 的不确定性：

$$
H(Y|X) = \sum_x \, p(x) \, H(Y|X = x)
$$

从单个属性（特征）$a$ 的角度来看，假设其可能取值为 $\{ a^1, a^2, \cdots, a^V\}$，$D^v$ 表示属性 $a$ 取值为 $a^v \in \{ a^1, a^2, \cdots, a^V\} $ 的样本集合，$\frac{|D^v|}{D}$ 表示占比，那么在已知 $a$ 的取值后，样本集合 $D$ 的条件熵为：

$$
\sum_{v=1}^V \frac{|D^v|}{D}Ent(D^v)
$$

### 信息增益

在已知属性（特征）$a$ 的取值后 $y$ 的不确定性减少的两，也即纯度的提升

$$
Gain(D, a) = Ent(D) - \sum_{v=1}^V \, \frac{|D^v|}{D}Ent(D^v)
$$

以信息增益为准则来选择划分属性的决策数为ID3决策树：

$$
a_* = arg \, max \, Gain(D, a)
$$


## C4.5决策树

信息增益准则对可能取值数目较多的属性由所偏好（本质原因不是取值数目过多，而是每个取值里所包含的样本量太少），为了减少这种偏好带来的影响，C4.5决策树使用“增益率”代替“信息增益”：

$$
Gain_ratio(D, a) = \frac{Gain(D,a)}{IV(a)}
$$

其中：

$$
IV(a) = - \sum_{v=1}^V \, \frac{|D^v|}{D} \, log_2 \, \frac{|D^v|}{D}
$$

称为属性 $a$ 的“固有值”，$a$ 的可能取值个数 $V$ 越大，通常其固有值 $IV(a)$ 也越大。但是，增益率对可能取值数目较少的属性有所偏好。


## CART决策树

### 基尼值

从样本集合 $D$ 中随机抽取两个样本，其类别标记不一致的概率。因此，基尼值越小，碰到异类的概率越小，纯度越高。

$$
\text{Gini}(D) = \sum_{k=1}^{|Y|} \sum_{k'\neq k} p_kp_{k'}  = \sum_{k=1}^{|Y|} p_k(1-p_k) = 1 - \sum_{k=1}^{|Y|} p_k^2
$$

极端情况

| 数据分布        | 计算过程                         | 基尼值       | 含义         |
| ----------- | ---------------------------- | --------- | ---------- |
| 纯节点（100%一类） | $1 - 1^2 = 0$                | **0**     | 完全纯净       |
| 两类各50%      | $1 - (0.5^2 + 0.5^2) = 0.5$  | **0.5**   | 最混乱        |
| 三类各33.3%    | $1 - 3\times(1/3)^2 = 0.667$ | **0.667** | 类别越多，最大值越大 |

规律：基尼值范围是 $[0, 1-\frac{1}{K}]$，$K$ 为类别数。二分类时最大值为 $0.5$。

### 基尼指数

$$
\text{Gini\_index}(D, a) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} \cdot \text{Gini}(D^v)
$$


### 算法

CART决策树选择基尼指数最小的属性作为最优划分属性：

$$
a_* = arg \; min \; Gini_index(D, a)
$$

步骤1：计算划分前的基尼值

$$
\text{Gini}(D) = 1 - \sum_{k=1}^{|Y|} p_k^2
$$

步骤2：尝试所有可能的划分

对每个属性 $a$ 的每个可能取值 $v$，将数据集 $D$ 分为 $a = v$ 和 $a \neq v$ 两部分计算：

$$
\text{Gini\_split}(D, a) = \frac{|D_{a=v}|}{|D|}\text{Gini}(D_{a=v}) + \frac{|D_{a\neq v}|}{|D|}\text{Gini}(D_{a\neq v})
$$

然后选择基尼指数最小的属性以及对应取值作为最优划分属性和最优划分点：

$$
\text{Best Split} = \arg\min_{a} \text{Gini\_split}(D, a)
$$


最后重复以上两部，直至满足停止条件。