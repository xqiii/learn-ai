# 机器学习基础

三要素

- 模型：根据具体问题，确定假设空间。
- 策略：根据评价标准，确定选取最优模型的策略（损失函数）。
- 算法：求解损失函数，确定最优模型。


## 基本术语

算法：从数据学得“模型”的具体方法，线性回归、对数几率
决策树等。

模型：算法产出的结果，通常是具体的函数或者可抽象看作为函数。

样本：关于一个事件或对象的描述。一般数据是向量的形式，向量的各个维度称为“特征”或者“属性”

样本空间：也称为“输入空间”或“属性空间”。表示样本的特征向量所在的空间为样本空间。

标记空间：标记所在空间称为“标记空间”或“输出空间”。

学习任务：标记取值为离散，称此类任务为“分类”。标记取值为连续时，称此类任务为“回归”。

监督学习：模型训练阶段有用到标记信息。（线性模型等）

无监督学习：模型训练阶段没用到标记信息。（聚类等）

数据集：数据集通常用集合来表示，令集合 $D = \{x_1,x_2,\cdots,x_m \}$ 表示包含 $m$ 个样本的数据集，一般同一份数据集的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有 $d$ 个特征，则第 $i$ 个样本的数据表示为 $d$ 维向量：$x_i = \{ x_{i1}; \cdots; x_{id} \}$，其中 $x_{ij}$ 表示样本 $x_i$ 在第 $j$ 个属性上的取值。

泛化：对未知物判断的准确与否才是衡量一个模型的关键，我们称此为“泛化”能力。

## 一般流程

流程：明确目标、收集数据、输入数据、数据探索与预处理、构建模型、训练模型、评估模型、优化模型。

选择模型和损失函数：

在实际选择时，一般会选用几种不同的方法来训练模型，然后比较性能，从中择优。

选择好模型还需要考虑：

- 最后一层是否需要添加 softmax 或 sigmoid 激活层
- 选择合适的损失函数
- 选择合适的优化器

选择损失函数：

1. 回归任务 → 首选MSE，有异常值用Huber/MAE 
2. 二分类 → 二元交叉熵 + Sigmoid 
3. 多分类 → 多分类交叉熵 + Softmax  
4. 类别不平衡 → Focal Loss 或 加权交叉熵  
5. 目标检测 → IoU系列损失 
6. 分割任务 → Dice Loss + BCE 组合  
7. 需要泛化 → 添加Label Smoothing  

评估以及优化模型：

1. 留出法。数据集划分为两个互斥的集合，其中一个作为训练集，另外一个作为测试集。
2. k折交叉验证
3. 重复k折交叉验证

## 过拟合与欠拟合

在训练时不仅最小化训练误差，还要限制模型参数的复杂度，让模型保持"简单、平滑"。

总损失 = 原始损失 + 正则化项

L(θ) = Loss(θ) + λ·R(θ)

### 权重正则化

L1正则化：

L1正则化通过恒定大小的梯度惩罚，将不重要的权重推向0，实现自动特征选择和模型压缩

$$
w = w + \lambda \sum_{i=1}^n |w_i|
$$

L2正则化：
$$
w = w + \frac{\lambda}{2} \sum_{i=1}^n w_i^2
$$

### dropout正则化

训练时随机"丢弃"部分神经元，防止神经元之间产生过度依赖

```text
训练时：                    测试时：
    输入                        输入
      │                          │
      ▼                          ▼
   ┌─────┐                   ┌─────┐
   │ ● ● │  随机丢弃50%        │ ● ● │  全部保留
   │ ●   │  ──────────→       │ ● ● │  
   │   ● │                   │ ● ● │
   └─────┘                   └─────┘
      │                          │
      ▼                          ▼
    输出                        输出（权重×p）
mask = (np.random.rand(*x.shape) < p) / p
out = x * mask
训练时：输出 × (1/p) × p = 输出 × 1 = 原输出
              └──────┘   └─┘
              保留概率  缩放补偿
```

### 批量归一化

在每一层的输入上进行标准化处理，使其保持稳定的分布（均值为0，方差为1），从而缓解"内部协变量偏移"

| 步骤 | 操作 | 公式 |
|------|------|------|
| 1 | 计算批次均值 |  $ \mu_B = \frac{1}{m}\sum_{i=1}^{m} x_i $  |
| 2 | 计算批次方差 |  $ \sigma_B^2 = \frac{1}{m}\sum_{i=1}^{m} (x_i - \mu_B)^2 $  |
| 3 | 标准化 |  $ \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} $  |
| 4 | 缩放和平移 |  $ y_i = \gamma \hat{x}_i + \beta $  |

```
原始输入 x          标准化 x̂            输出 y
    │                   │                  │
    ▼                   ▼                  ▼
┌─────────┐        ┌─────────┐        ┌─────────┐
│ 任意分布 │  ──→   │ N(0,1)  │  ──→   │ N(β,γ²) │
│ 不稳定   │        │ 稳定    │        │ 可学习  │
└─────────┘        └─────────┘        └─────────┘
     │                   │                  │
     └───────────────────┴──────────────────┘
              解决协变量偏移 + 保持表达能力

pytorch: nn.BatchNorm2d

```

### 层归一化

在单个样本的所有特征上进行标准化，而不是像 BN 那样在批次维度上标准化。


```
批量归一化 (BN)：          层归一化 (LN)：
                            
样本1  [x₁₁, x₁₂, x₁₃]      样本1  [x₁₁, x₁₂, x₁₃]
样本2  [x₂₁, x₂₂, x₂₃]      样本2  [x₂₁, x₂₂, x₂₃]
样本3  [x₃₁, x₃₂, x₃₃]      样本3  [x₃₁, x₃₂, x₃₃]
  │                          │
  ▼                          ▼
按列计算均值方差            按行计算均值方差
(每个特征独立)              (每个样本独立)


pytorch: nn.LayerNorm
```


### 权重初始化

权重初始化是指在神经网络训练开始前，为网络的权重参数和偏置赋予初始值的过程。这是深度学习模型训练的关键步骤之一，直接影响模型的收敛速度、训练稳定性和最终性能。

| 目的 | 说明 |
|------|------|
| 打破对称性 | 避免同一层神经元学习相同特征 |
| 防止梯度消失 | 确保梯度能有效反向传播到浅层 |
| 防止梯度爆炸 | 避免梯度过大导致训练不稳定 |
| 加速收敛 | 为模型提供良好的训练起点 |

1. 零初始化。将所有权重和偏置设为0。问题：对称性，同层所有神经元输出相同，梯度同步更新。梯度消失，多层叠加后输出趋于0.
2. 随机初始化。从均匀分布或高斯分布中随机采样小值。问题：深层网络中易导致梯度消失或爆炸。
3. Xavier初始化（Glorot初始化）。保持每层输出的方差一致，Sigmoid、Tanh等对称激活函数。权重方差 = 2 / (fan_in + fan_out)。（torch.nn.init.xavier_normal_()）
4. He初始化（Kaiming初始化）。针对ReLU激活函数优化，调整权重初始化范围。权重方差 = 2 / fan_in。（torch.nn.init.kaiming_normal_()）

## 激活函数

1. Sigmoid
- 公式：$f(x) = \frac{1} {1 + e^{-x}}$
- 输出范围：(0, 1)
- 优点：
    - 输出可表示概率，适合二分类
    - 平滑连续可微
- 缺点：
    - 梯度消失（输入绝对值大时导数接近0）
    - 输出非零均值，影响训练稳定性
2.  Tanh（双曲正切）函数
- 公式：$f(x) = \frac{e^x - e^{-x}}{ e^x + e^{-x}}$
- 输出范围：(-1, 1)
- 优点:
    - 零中心输出，梯度更新更稳定
    - 比Sigmoid收敛更快
- 缺点：
    - 仍存在梯度消失问题
3. ReLU
- 公式：$f(x) = max(0, x)$
- 输出范围：$[0, +\infty)$
- 优点：
    - 计算简单高效
    - 缓解梯度消失问题
    - 稀疏激活，减少参数依赖
- 缺点：
    - "死亡ReLU"问题（负输入梯度为0）
    - 输出非零中心
4. Leaky ReLU
- 公式：$f(x) = max(\alpha x, x)$，（α通常为0.01）
- 输出范围：$[-\infty, +\infty)$
- 优点：
    - 解决死亡ReLU问题，负区间有微小梯度
- 缺点：
    - α参数需要调整
5. Softmax
- 公式：$f(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}$
- 输出范围：(0, 1)，且和为1
- 优点：
    - 输出可解释为概率分布
- 缺点：
    - 计算量大，需注意数值稳定性

```
┌─────────────────────────────────────────────────┐
│              激活函数选择流程图                  │
├─────────────────────────────────────────────────┤
│  1. 确定层类型                                   │
│     ├─ 隐藏层 → ReLU/Leaky ReLU/GELU            │
│     └─ 输出层 → 根据任务选择                    │
│                                                  │
│  2. 确定任务类型                                 │
│     ├─ 二分类 → Sigmoid                         │
│     ├─ 多分类 → Softmax                         │
│     └─ 回归 → 线性函数                          │
│                                                  │
│  3. 考虑网络深度                                 │
│     ├─ 深层网络 → ReLU系列/GELU                 │
│     └─ 浅层网络 → 选择更灵活                    │
│                                                  │
│  4. 实验验证                                     │
│     └─ 根据实际效果调整                         │
└─────────────────────────────────────────────────┘
```

### SwiGLU

SwiGLU（Swish-Gated Linear Unit）是一种结合了 Swish 激活函数 和 GLU（门控线性单元）机制 的激活结构

GLU（Gated Linear Unit）

将输入分为两部分，一部分作为"门"控制信息流动，另一部分作为实际信息传递。

```
GLU(x) = (xW + b) ⊗ σ(xV + c)
              │         │
              │         └─ 门控信号（Sigmoid）
              └─ 信息流（线性变换）
```
Swish

平滑、非单调、自门控特性

```
Swish(x) = x · σ(x)
           │   │
           │   └─ Sigmoid 门控
           └─ 原始输入
```

SwiGLU

```
SwiGLU(x) = Swish(xW) ⊗ (xV)
          = (xW · σ(xW)) ⊗ (xV)
```

用 Swish 替代 GLU 中的 Sigmoid 门控，获得更强的表达能力

## 损失函数

损失函数作用

| 作用 | 说明 |
|------|------|
| 量化预测误差 | 将预测值与真实值的差异转化为可比较的数值 |
| 指导模型优化 | 提供梯度，告诉优化器如何调整参数 |
| 评估模型性能 | 在训练和验证过程中衡量模型表现 |

回归任务


| 损失函数 | 公式 | 特点 | 适用场景 |
|----------|------|------|----------|
| MSE (均方误差) |  $ \frac{1}{n}\sum(y-\hat{y})^2 $  | 对异常值敏感，可导 | 数据干净、无离群点 |
| MAE (平均绝对误差) |  $\frac{1}{n}\sum y-\hat{y}$||   | 对异常值鲁棒，不可导 | 存在离群点的数据 |
| Huber Loss | 分段函数 | 兼顾MSE和MAE优点 | 有少量异常值 |
| Log-Cosh Loss |  $ \sum\log(\cosh(y-\hat{y})) $  | 平滑、可导 | 需要二阶可导的场景 |
| Quantile Loss | 分位数损失 | 预测置信区间 | 需要不确定性估计 |

分类任务

| 损失函数 | 适用场景 | 特点 |
|----------|----------|------|
| 交叉熵损失 (Cross-Entropy) | 多分类/二分类 | 最常用，与Softmax配合 |
| BCE Loss (二元交叉熵) | 二分类 | 输出为概率值 |
| Hinge Loss | SVM、最大间隔分类 | 不可导，需次梯度 |
| Focal Loss | 类别不平衡 | 降低易分类样本权重 |






## 优化器

## GPU加速

## 数学

### 极大似然估计（MLE）

使得观测样本出现概率最大的分布

设样本 $X_1, ..., X_n$ 独立同分布，概率密度（或质量）函数为 $f(x; \theta)$ ，其中 $\theta$ 是待估参数。

似然函数：

$$
L(\theta) = L(\theta; x_1, ..., x_n) = \prod_{i=1}^{n} f(x_i; \theta)
$$

对数似然：

方便求导，单调递增

$$
\ell(\theta) = \ln L(\theta) = \sum_{i=1}^{n} \ln f(x_i; \theta)
$$

### 凸集

集合 $C \subseteq \mathbb{R}^n$ 是凸集，当且仅当：

$$
\forall x, y \in C, \quad \forall \theta \in [0, 1], \quad \theta x + (1-\theta)y \in C
$$

$\theta x + (1-\theta)y$ 就是 $x$ 和 $y$  之间的凸组合（线段上的任意点）。

| 凸集                             | 非凸集      |
| ------------------------------ | -------- |
| 整个空间 $\mathbb{R}^n$            | 两个分离的圆盘  |
| 半空间 $\{x \mid a^Tx \leq b\}$   | 圆环（中间有洞） |
| 球体 $\{x \mid \|x\|_2 \leq r\}$ | 月牙形区域    |
| 单纯形（概率单纯形）                     | 任意非连通集合  |

### 凸函数

函数 $f: \mathbb{R}^n \to \mathbb{R}$ 是凸函数，当且仅当（最优化）：

$$
\forall x, y \in \text{dom}(f), \quad \forall \theta \in [0, 1]:
$$

$$
f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)
$$

### 梯度

对于函数 $f: \mathbb{R}^n \to \mathbb{R}$ （输入向量，输出标量），其梯度为：

$$
\nabla f(x) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
$$

所有偏导数组成的列向量。

### Hessian 矩阵

对于 $f: \mathbb{R}^n \to \mathbb{R}$，Hessian 矩阵 $\mathbf{H} \in \mathbb{R}^{n \times n}$：

$$
\mathbf{H} = \nabla^2 f(x) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\[6pt]
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\[6pt]
\vdots & \vdots & \ddots & \vdots \\[6pt]
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

所有二阶偏导数组成的对称矩阵（若 $f$ 二阶连续可微，则$\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$）

设 $D \in \mathbb{R}^n$ 是非空开凸集，$f(x)$ 是定义在$D$上的实值函数，且 $f(x)$ 在 $D$ 上二阶连续可微，如果 $f(x)$ 的 Hessian 矩阵 $\nabla^2f(x)$ 在 $D$ 上是半正定的，则 $f(x)$ 是 $D$ 上的凸函数；如果 $\nabla^2f(x)$ 在
$D$ 上是正定的，则 $f(x)$ 是 $D$ 上的严格凸函数。


证正定：顺序主子式全 $> 0$（西尔维斯特）

证正半定：所有主子式 $\ge 0$，或直接用二次型 $z^\top H z \geq 0$（更常用）


### 矩阵微分

#### 标量求导

$$
(uv)' = u'v + uv'
$$

#### 迹

是方阵对角线元素之和，记作 $\text{tr}(A)$ 。在矩阵求导中，它是把标量包装成矩阵形式的利器，核心作用是让乘法顺序可以轮换。

$$
\text{tr}(A) = \sum_{i=1}^n A_{ii} = A_{11} + A_{22} + \dots + A_{nn}
$$

性质

$$
\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)
$$

套路：

1. 标量 $a = \text{tr}(a)$ （包装）
2. $\text{tr}(ABC) = \text{tr}(CAB)$ （轮换，把 $dx$ 转到合适位置）
3. 或直接利用 $u^\top v = v^\top u$ （标量转置等于自身）


#### 矩阵求导 

| 函数形式                        | 导数                                    | 备注                      |
| --------------------------- | ----------------------------------------------- | ----------------------- |
| $f(x) = x^\top A x$         | $\frac{\partial f}{\partial x} = (A + A^\top)x$ | **一般情况**                |
| $f(x) = x^\top A x$（$A$ 对称） | $\frac{\partial f}{\partial x} = 2Ax$           | **最常见**（如 $A=X^\top X$） |
| $f(x) = a^\top x$           | $\frac{\partial f}{\partial x} = a$             | 线性项                     |


### 拉格朗日乘法

#### 拉格朗日乘子

对于仅含等式约束的优化问题：

$$
min \; f(x) \\
s.t. \; h_i(x) = 0 \; i = 1,2,3,\cdots,n
$$

其中自变量 $x \in \mathbb{R}^n$，$f(x)$ 和 $h_i(x)$ 均有连续的一阶偏导数，其拉格朗日函数：

$$
L(x, \lambda) = f(x) + \sum_{i=1}^n \lambda_i h_i(x)
$$

其中 $\lambda = (\lambda_1,\lambda_2,\cdots,\lambda_n)^T$ 为拉格朗日乘子。然后对拉格朗日函数关于 $x$ 求偏导，并令导数等于 $0$ 再搭配约束条件 $h_i(x) = 0$解出 $x$，求解出的所有 $x$ 即为上述优化问题的所有可能的极值点。

在约束曲面上，只有当目标函数的梯度完全垂直约束曲面时，才不能再沿着约束移动改善目标函数值。而垂直于同一个曲面的两个向量，必然共线。

所有 $\lambda \ge 0$，如果 $\lambda_i = 0$，那么对应的约束条件 $g_i(x)$ 是松弛的；如果 $\lambda_i > 0$，那么对应的约束条件 $g_i(x)$ 是紧致的。


#### 对偶问题

原始问题：通常是指最初的优化问题

原始问题（primal problem）在满足一定条件时，通过一系列变换和处理，可以生成一个与之相关的对偶问题。对偶问题和原始问题是等价的，对偶问题的解就是原始问题的解。

### 广义特征值

设 $A$、$B$ 为 $n$ 阶方阵，若存在 $\lambda$，使得方程 $Ax = \lambda Bx$ 存在非零解，则称 $\lambda$ 为 $A$ 相对与 $B$ 的广义特征向值，$x$ 为 $A$ 相对与 $B$ 的广义特征向值 $\lambda$ 的特征向量。