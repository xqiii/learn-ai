{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff808273",
   "metadata": {},
   "source": [
    "# 深度学习框架\n",
    "\n",
    "## 包内容\n",
    "\n",
    "- torch：类似于 Numpy 的通用数组库\n",
    "- torch.autograd：用于构建计算图形并自动获取梯度\n",
    "- torch.nn：共享层和损失函数的神经网络库\n",
    "- torch.optim：具有通用优化算法（SGD、Adam等）的优化包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2ea0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建Tensor\n",
    "import torch\n",
    "\n",
    "# 根据列表数据生成\n",
    "torch.Tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# 根据维度生成\n",
    "torch.Tensor(2,3)\n",
    "\n",
    "# 根据已有的张量生成\n",
    "t = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(t.size())\n",
    "\n",
    "print(t.shape)\n",
    "\n",
    "torch.Tensor(t.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65812603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 0.4929, -0.4592,  1.0768],\n",
      "        [-0.7646, -1.3746,  1.0486]])\n",
      "tensor([ 1.,  4.,  7., 10.])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建单位矩阵  \n",
    "print(torch.eye(2,2))\n",
    "\n",
    "# 创建全0矩阵\n",
    "print(torch.zeros(2,3))\n",
    "\n",
    "# 创建全1矩阵\n",
    "print(torch.ones(2,3))\n",
    "\n",
    "# 创建随机矩阵\n",
    "print(torch.randn(2,3))\n",
    "\n",
    "# 创建等差数列\n",
    "print(torch.linspace(1,10,4))\n",
    "\n",
    "# 创建与已有张量形状相同的全0矩阵\n",
    "print(torch.zeros_like(torch.randn(2,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d960a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5588,  1.8407],\n",
      "        [ 0.0265, -1.1522],\n",
      "        [-0.2922,  1.1332]])\n",
      "tensor([[-2.0460,  0.0788],\n",
      "        [ 0.4230, -0.2723],\n",
      "        [-1.9369, -1.1090]])\n"
     ]
    }
   ],
   "source": [
    "# 修改张量形状\n",
    "print(torch.reshape(torch.randn(2,3), (3,2)))\n",
    "\n",
    "# 转置\n",
    "print(torch.t(torch.randn(2,3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b137bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵加法\n",
      "tensor([[ 0.8654,  3.0735, -0.2771],\n",
      "        [ 0.4369, -0.0611, -1.3940]])\n",
      "矩阵乘法\n",
      "tensor([[ 2.1183, -0.6892],\n",
      "        [ 1.2280, -0.5641]])\n",
      "广播\n",
      "tensor([[-2.5558, -0.0144, -0.9916],\n",
      "        [ 0.1632,  0.9256, -0.1912]])\n",
      "bmm\n",
      "tensor([[[39, 49, 33, 26],\n",
      "         [42, 42, 33, 36]],\n",
      "\n",
      "        [[74, 52, 44, 16],\n",
      "         [62, 36, 46, 14]]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵操作\n",
    "A = torch.randn(2,3)\n",
    "B = torch.randn(2,3)\n",
    "\n",
    "print(\"矩阵加法\")\n",
    "print(A+B)\n",
    "\n",
    "print(\"矩阵乘法\")\n",
    "print(A.mm(B.T))\n",
    "\n",
    "# 广播\n",
    "A = torch.randn(2,3)\n",
    "B = torch.randn(2,1)\n",
    "\n",
    "print(\"广播\")\n",
    "print(A+B)\n",
    "\n",
    "# bmm\n",
    "# 对批量的3维张量进行矩阵乘法\n",
    "A = torch.randint(10,(2,2,3))\n",
    "B = torch.randint(6,(2,3,4))\n",
    "\n",
    "print(\"bmm\")\n",
    "print(torch.bmm(A,B))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6376cfa",
   "metadata": {},
   "source": [
    "# autograd\n",
    "\n",
    "## 符号微分\n",
    "\n",
    "将原表达式转换为求导表达式\n",
    "\n",
    "优：精确数值结果\n",
    "\n",
    "缺：表达式膨胀\n",
    "\n",
    "## 数值微分\n",
    "\n",
    "使用有限差分来近似：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x_i} \\approx \\frac{f(x + he_i) - f(x)}{h}, \\; where h > 0\n",
    "$$\n",
    "\n",
    "优：容易实现\n",
    "\n",
    "缺：计算结果不精确，计算复杂度高，对h要求高\n",
    "\n",
    "## 自动微分\n",
    "\n",
    "所有数值计算由有限的基本运算组成，基本运算的导数表达式是已知的，通过链式法则将数值计算部分组合成整体\n",
    "\n",
    "\n",
    "### OO模式\n",
    "\n",
    "1. 利用语言多态性，重载基本运算操作符\n",
    "2. 将表达式操作类型和输入输出信息，记录到 Tape 中\n",
    "3. 对 Tape 遍历，并对其中记录的基本运算操作进行微分\n",
    "4. 把结果通过链式法则进行组合\n",
    "\n",
    "优：\n",
    "- 实现简单\n",
    "- 语言具备多态性\n",
    "- 易用性高，贴合原生语言\n",
    "\n",
    "缺：\n",
    "- 显式的构造 Tape 数据结构和对 Tape 进行读写\n",
    "- 额外数据结构和操作的引入，不利于高阶微分\n",
    "- if, while通常难以通过操作符重载\n",
    "\n",
    "参考： https://zhuanlan.zhihu.com/p/69294347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45e1bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向结果: x=2.0, y=4.0, z=7.0, loss=14.0\n"
     ]
    }
   ],
   "source": [
    "# 前向传播：\n",
    "#     x = 2\n",
    "#     │\n",
    "#     ▼  (平方)\n",
    "#     y = x² = 4\n",
    "#     │\n",
    "#     ▼  (加3)\n",
    "#     z = y + 3 = 7\n",
    "#     │\n",
    "#     ▼  (乘2)\n",
    "#     loss = 2z = 14\n",
    "import torch\n",
    "\n",
    "# 创建输入张量（叶子节点）\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# 前向计算\n",
    "y = x ** 2        # y = 4\n",
    "z = y + 3         # z = 7\n",
    "loss = 2 * z      # loss = 14\n",
    "\n",
    "print(f\"前向结果: x={x.item()}, y={y.item()}, z={z.item()}, loss={loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacc3d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "反向传播结果:\n",
      "x.grad = 8.0\n",
      "\n",
      "计算图结构:\n",
      "loss.grad_fn: <MulBackward0 object at 0x104c8f070>\n",
      "z.grad_fn: <AddBackward0 object at 0x104cc3fa0>\n",
      "y.grad_fn: <PowBackward0 object at 0x104c8f070>\n",
      "x.grad_fn: None\n"
     ]
    }
   ],
   "source": [
    "# 反向传播\n",
    "# 目标：计算 d(loss)/d(x)\n",
    "\n",
    "# 根据链式法则：\n",
    "# d(loss)/d(x) = d(loss)/d(z) × d(z)/d(y) × d(y)/d(x)\n",
    "\n",
    "# 逐步计算：\n",
    "# ┌─────────────────────────────────────────────┐\n",
    "# │ ① d(loss)/d(z) = d(2z)/d(z) = 2            │\n",
    "# │ ② d(z)/d(y) = d(y+3)/d(y) = 1              │\n",
    "# │ ③ d(y)/d(x) = d(x²)/d(x) = 2x = 2×2 = 4    │\n",
    "# └─────────────────────────────────────────────┘\n",
    "\n",
    "# 最终梯度：\n",
    "# d(loss)/d(x) = 2 × 1 × 4 = 8\n",
    "\n",
    "\n",
    "# 反向传播（从 loss 向 x 传播）：\n",
    "\n",
    "#     loss = 14\n",
    "#       │\n",
    "#       │ d(loss)/d(loss) = 1 (初始梯度)\n",
    "#       ▼\n",
    "#     loss = 2z\n",
    "#       │\n",
    "#       │ d(loss)/d(z) = 2\n",
    "#       ▼\n",
    "#     z = y + 3\n",
    "#       │\n",
    "#       │ d(loss)/d(y) = 2 × 1 = 2\n",
    "#       ▼\n",
    "#     y = x²\n",
    "#       │\n",
    "#       │ d(loss)/d(x) = 2 × 2x = 2 × 4 = 8\n",
    "#       ▼\n",
    "#     x = 2 (叶子节点，梯度存储在这里)\n",
    "import torch\n",
    "\n",
    "# 创建计算图\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y + 3\n",
    "loss = 2 * z\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(f\"\\n反向传播结果:\")\n",
    "print(f\"x.grad = {x.grad.item()}\")  # 应该等于 8\n",
    "\n",
    "# 验证计算图结构\n",
    "print(f\"\\n计算图结构:\")\n",
    "print(f\"loss.grad_fn: {loss.grad_fn}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")\n",
    "print(f\"x.grad_fn: {x.grad_fn}\")  # None (叶子节点)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb00ae",
   "metadata": {},
   "source": [
    "# 神经网络组件\n",
    "\n",
    "## 基本模块\n",
    "\n",
    "- 层：神经网络基本结构，将输入张量转换为输出张量\n",
    "- 模型：由层构成的网络\n",
    "- 损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数\n",
    "- 优化器：在使损失函数最小值时，涉及优化器\n",
    "\n",
    "## 训练模型\n",
    "\n",
    "1. 加载和预处理数据集（torch.utils、torchvision等）。\n",
    "2. 定义损失函数（回归nn.MSELoss()，分类nn.BCELoss()等）。\n",
    "3. 定义优化方法（torch.Optimizer）\n",
    "4. 循环训练模型\n",
    "\n",
    "    ```\n",
    "    # 设置为训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 求损失值\n",
    "    y_prev = model(x)\n",
    "    loss = loss_fn(f_prev,t_true)\n",
    "\n",
    "    # 自动求导，反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    # 循环验证模型\n",
    "    model.eval()\n",
    "\n",
    "    # 不跟踪梯度计算损失\n",
    "    with.torch.no_grad():\n",
    "\n",
    "    ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
